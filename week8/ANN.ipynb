{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614, 0.2550690257394217]}, {'weights': [0.49543508709194095, 0.4494910647887381, 0.651592972722763, 0.7887233511355132]}]\n",
            "[{'weights': [0.0938595867742349, 0.02834747652200631, 0.8357651039198697]}, {'weights': [0.43276706790505337, 0.762280082457942, 0.0021060533511106927]}]\n"
          ]
        }
      ],
      "source": [
        "from math import exp\n",
        "from random import seed\n",
        "from random import random\n",
        "#here we basically initialised a network with all the layers we need\n",
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "\tnetwork = list()\n",
        "\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
        "\t#gets all inputs and bias fr each neuron in hidden layer\n",
        "\tnetwork.append(hidden_layer)\n",
        "\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
        "\t#each neuron in output layer gets input from each neuron in hidden layer\n",
        "\tnetwork.append(output_layer)    \n",
        "\treturn network\n",
        "seed(1)\n",
        "network=initialize_network(3,2,2)\n",
        "for i in network:\n",
        "\tprint(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def activate(weights, inputs):\n",
        "\tactivation = weights[-1]\n",
        "\tfor i in range(len(weights)-1):#last weight is bias so nt necessary to multiply with an input\n",
        "\t\tactivation += weights[i] * inputs[i]\n",
        "\treturn activation\n",
        "#used in activating each neuron i.e calculating actual value of a particular neuron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transfer(activation):\n",
        "\treturn 1.0 / (1.0 + exp(-activation))\n",
        "#passing neuron value through the activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.6629970129852887, 0.7253160725279748]\n"
          ]
        }
      ],
      "source": [
        "def forward_propagate(network, row):\n",
        "\tinputs = row #row from our input data\n",
        "\tfor layer in network:\n",
        "\t\tnew_inputs = []\n",
        "\t\tfor neuron in layer:\n",
        "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
        "\t\t\tneuron['output'] = transfer(activation)\n",
        "\t\t\tnew_inputs.append(neuron['output'])\n",
        "\t\tinputs = new_inputs\n",
        "\treturn inputs\n",
        "# as each layers output is input to other layers this function basically calculates the inputs of next layer and stores the output of present neuron in the neuron itself\n",
        "# test forward propagation\n",
        "network = [[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
        "\t\t[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
        "row = [1, 0, None]\n",
        "output = forward_propagate(network, row)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backward propagation\n",
        "The backpropagation algorithm is named for the way in which weights are trained.\n",
        "\n",
        "Error is calculated between the expected outputs and the outputs forward propagated from the network. These errors are then propagated backward through the network from the output layer to the hidden layer, assigning blame for the error and updating weights as they go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#calculates slope\n",
        "def transfer_derivative(output):\n",
        "\treturn output * (1.0 - output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614], 'delta': -0.0005348048046610517}]\n",
            "[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095], 'delta': -0.14619064683582808}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763], 'delta': 0.0771723774346327}]\n"
          ]
        }
      ],
      "source": [
        "# Backpropagate error and store in neurons\n",
        "def backward_propagate_error(network, expected):\n",
        "\tfor i in reversed(range(len(network))):\n",
        "\t\tlayer = network[i]\n",
        "\t\terrors = list()\n",
        "\t\tif i != len(network)-1:#for hidden layers\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\terror = 0.0\n",
        "\t\t\t\tfor neuron in network[i + 1]:\n",
        "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
        "\t\t\t\terrors.append(error)\n",
        "\t\telse:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\tneuron = layer[j]\n",
        "\t\t\t\terrors.append(expected[j] - neuron['output'])\n",
        "\t\tfor j in range(len(layer)):\n",
        "\t\t\tneuron = layer[j]\n",
        "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])#this part dint understand\n",
        "# test backpropagation of error\n",
        "network = [[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
        "\t\t[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095]}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
        "expected = [0, 1]\n",
        "backward_propagate_error(network, expected)\n",
        "for layer in network:\n",
        "\tprint(layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update network weights with error\n",
        "def update_weights(network, row, l_rate):\n",
        "\tfor i in range(len(network)):\n",
        "\t\tinputs = row[:-1]\n",
        "\t\tif i != 0:\n",
        "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
        "\t\tfor neuron in network[i]:\n",
        "\t\t\tfor j in range(len(inputs)):\n",
        "\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]#weight is a given weight, learning_rate is a parameter that you must specify, error is the error calculated by the backpropagation procedure for the neuron and input is the input value that caused the error.\n",
        "\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']#updating for bias\n",
        "\n",
        " #Learning rate controls how much to change the weight to correct for the error. For example, a value of 0.1 will update the weight 10% of the amount that it possibly could be updated. Small learning rates are preferred that cause slower learning over a large number of training iterations. This increases the likelihood of the network finding a good set of weights across all layers rather than the fastest set of weights that minimize error (called premature convergence)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2yKQtJcZ7al",
        "outputId": "95d03a8c-58e3-4ffd-f13b-4120ee353f3c"
      },
      "outputs": [],
      "source": [
        "# Train a network for a fixed number of epochs\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "\tfor epoch in range(n_epoch):\n",
        "\t\tsum_error = 0\n",
        "\t\tfor row in train:\n",
        "\t\t\toutputs = forward_propagate(network, row)\n",
        "\t\t\texpected = [0 for i in range(n_outputs)]\n",
        "      # print('+++')\n",
        "\t\t\texpected[row[-1]] = 1\n",
        "      #print(expected)\n",
        "\t\t\tsum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])#squared error\n",
        "\t\t\tbackward_propagate_error(network, expected)\n",
        "\t\t\tupdate_weights(network, row, l_rate)\n",
        "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "e4JxxT6Lp5SC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}, {'weights': [0.2550690257394217, 0.49543508709194095, 0.4494910647887381]}], [{'weights': [0.651592972722763, 0.7887233511355132, 0.0938595867742349]}, {'weights': [0.02834747652200631, 0.8357651039198697, 0.43276706790505337]}]]\n",
            ">epoch=0, lrate=0.500, error=6.350\n",
            ">epoch=1, lrate=0.500, error=5.531\n",
            ">epoch=2, lrate=0.500, error=5.221\n",
            ">epoch=3, lrate=0.500, error=4.951\n",
            ">epoch=4, lrate=0.500, error=4.519\n",
            ">epoch=5, lrate=0.500, error=4.173\n",
            ">epoch=6, lrate=0.500, error=3.835\n",
            ">epoch=7, lrate=0.500, error=3.506\n",
            ">epoch=8, lrate=0.500, error=3.192\n",
            ">epoch=9, lrate=0.500, error=2.898\n",
            ">epoch=10, lrate=0.500, error=2.626\n",
            ">epoch=11, lrate=0.500, error=2.377\n",
            ">epoch=12, lrate=0.500, error=2.153\n",
            ">epoch=13, lrate=0.500, error=1.953\n",
            ">epoch=14, lrate=0.500, error=1.774\n",
            ">epoch=15, lrate=0.500, error=1.614\n",
            ">epoch=16, lrate=0.500, error=1.472\n",
            ">epoch=17, lrate=0.500, error=1.346\n",
            ">epoch=18, lrate=0.500, error=1.233\n",
            ">epoch=19, lrate=0.500, error=1.132\n",
            ">epoch=20, lrate=0.500, error=1.042\n",
            ">epoch=21, lrate=0.500, error=0.961\n",
            ">epoch=22, lrate=0.500, error=0.887\n",
            ">epoch=23, lrate=0.500, error=0.821\n",
            ">epoch=24, lrate=0.500, error=0.761\n",
            ">epoch=25, lrate=0.500, error=0.707\n",
            ">epoch=26, lrate=0.500, error=0.658\n",
            ">epoch=27, lrate=0.500, error=0.613\n",
            ">epoch=28, lrate=0.500, error=0.573\n",
            ">epoch=29, lrate=0.500, error=0.536\n",
            ">epoch=30, lrate=0.500, error=0.503\n",
            ">epoch=31, lrate=0.500, error=0.472\n",
            ">epoch=32, lrate=0.500, error=0.445\n",
            ">epoch=33, lrate=0.500, error=0.420\n",
            ">epoch=34, lrate=0.500, error=0.397\n",
            ">epoch=35, lrate=0.500, error=0.376\n",
            ">epoch=36, lrate=0.500, error=0.356\n",
            ">epoch=37, lrate=0.500, error=0.339\n",
            ">epoch=38, lrate=0.500, error=0.322\n",
            ">epoch=39, lrate=0.500, error=0.307\n",
            ">epoch=40, lrate=0.500, error=0.293\n",
            ">epoch=41, lrate=0.500, error=0.280\n",
            ">epoch=42, lrate=0.500, error=0.268\n",
            ">epoch=43, lrate=0.500, error=0.257\n",
            ">epoch=44, lrate=0.500, error=0.247\n",
            ">epoch=45, lrate=0.500, error=0.237\n",
            ">epoch=46, lrate=0.500, error=0.228\n",
            ">epoch=47, lrate=0.500, error=0.220\n",
            ">epoch=48, lrate=0.500, error=0.212\n",
            ">epoch=49, lrate=0.500, error=0.204\n",
            ">epoch=50, lrate=0.500, error=0.197\n",
            ">epoch=51, lrate=0.500, error=0.191\n",
            ">epoch=52, lrate=0.500, error=0.185\n",
            ">epoch=53, lrate=0.500, error=0.179\n",
            ">epoch=54, lrate=0.500, error=0.173\n",
            ">epoch=55, lrate=0.500, error=0.168\n",
            ">epoch=56, lrate=0.500, error=0.163\n",
            ">epoch=57, lrate=0.500, error=0.158\n",
            ">epoch=58, lrate=0.500, error=0.154\n",
            ">epoch=59, lrate=0.500, error=0.150\n",
            ">epoch=60, lrate=0.500, error=0.146\n",
            ">epoch=61, lrate=0.500, error=0.142\n",
            ">epoch=62, lrate=0.500, error=0.138\n",
            ">epoch=63, lrate=0.500, error=0.135\n",
            ">epoch=64, lrate=0.500, error=0.131\n",
            ">epoch=65, lrate=0.500, error=0.128\n",
            ">epoch=66, lrate=0.500, error=0.125\n",
            ">epoch=67, lrate=0.500, error=0.122\n",
            ">epoch=68, lrate=0.500, error=0.119\n",
            ">epoch=69, lrate=0.500, error=0.117\n",
            ">epoch=70, lrate=0.500, error=0.114\n",
            ">epoch=71, lrate=0.500, error=0.112\n",
            ">epoch=72, lrate=0.500, error=0.109\n",
            ">epoch=73, lrate=0.500, error=0.107\n",
            ">epoch=74, lrate=0.500, error=0.105\n",
            ">epoch=75, lrate=0.500, error=0.103\n",
            ">epoch=76, lrate=0.500, error=0.101\n",
            ">epoch=77, lrate=0.500, error=0.099\n",
            ">epoch=78, lrate=0.500, error=0.097\n",
            ">epoch=79, lrate=0.500, error=0.095\n",
            ">epoch=80, lrate=0.500, error=0.093\n",
            ">epoch=81, lrate=0.500, error=0.092\n",
            ">epoch=82, lrate=0.500, error=0.090\n",
            ">epoch=83, lrate=0.500, error=0.088\n",
            ">epoch=84, lrate=0.500, error=0.087\n",
            ">epoch=85, lrate=0.500, error=0.085\n",
            ">epoch=86, lrate=0.500, error=0.084\n",
            ">epoch=87, lrate=0.500, error=0.083\n",
            ">epoch=88, lrate=0.500, error=0.081\n",
            ">epoch=89, lrate=0.500, error=0.080\n",
            ">epoch=90, lrate=0.500, error=0.079\n",
            ">epoch=91, lrate=0.500, error=0.077\n",
            ">epoch=92, lrate=0.500, error=0.076\n",
            ">epoch=93, lrate=0.500, error=0.075\n",
            ">epoch=94, lrate=0.500, error=0.074\n",
            ">epoch=95, lrate=0.500, error=0.073\n",
            ">epoch=96, lrate=0.500, error=0.072\n",
            ">epoch=97, lrate=0.500, error=0.071\n",
            ">epoch=98, lrate=0.500, error=0.070\n",
            ">epoch=99, lrate=0.500, error=0.069\n",
            "[{'weights': [-1.8904818823786256, 2.6029577588462405, 1.36922598873631], 'output': 0.018283261957935986, 'delta': -0.0006070497524612589}, {'weights': [0.9538894079909441, -1.386542269801398, -0.19848619686044763], 'output': 0.9010494694007615, 'delta': 0.001321884937561888}]\n",
            "[{'weights': [4.447262579901695, -1.624718015475665, -1.26932683693348], 'output': 0.0660956654307449, 'delta': -0.004079889019937295}, {'weights': [-4.158762896273457, 2.1783932549616014, 0.808710771335537], 'output': 0.9365588447655114, 'delta': 0.0037694434734618643}]\n"
          ]
        }
      ],
      "source": [
        "seed(1)\n",
        "dataset = [[2.7810836,2.550537003,0],\n",
        "\t[1.465489372,2.362125076,0],\n",
        "\t[3.396561688,4.400293529,0],\n",
        "\t[1.38807019,1.850220317,0],\n",
        "\t[3.06407232,3.005305973,0],\n",
        "\t[7.627531214,2.759262235,1],\n",
        "\t[5.332441248,2.088626775,1],\n",
        "\t[6.922596716,1.77106367,1],\n",
        "\t[8.675418651,-0.242068655,1],\n",
        "\t[7.673756466,3.508563011,1]]\n",
        "n_inputs = len(dataset[0]) - 1\n",
        "n_outputs = len(set([row[-1] for row in dataset]))\n",
        "network = initialize_network(n_inputs, 2, n_outputs)\n",
        "print(network)\n",
        "train_network(network, dataset, 0.5, 100, n_outputs)\n",
        "for layer in network:\n",
        "\tprint(layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqJy54ckxocz",
        "outputId": "97bd001c-bc97-46dd-976e-7a26271d82f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n"
          ]
        }
      ],
      "source": [
        "# Make a prediction with a network\n",
        "def predict(network, row):\n",
        "\toutputs = forward_propagate(network, row)\n",
        "\treturn outputs.index(max(outputs))#try to understand\n",
        "\n",
        "# Test making predictions with the network\n",
        "dataset = [[2.7810836,2.550537003,0],\n",
        "\t[1.465489372,2.362125076,0],\n",
        "\t[3.396561688,4.400293529,0],\n",
        "\t[1.38807019,1.850220317,0],\n",
        "\t[3.06407232,3.005305973,0],\n",
        "\t[7.627531214,2.759262235,1],\n",
        "\t[5.332441248,2.088626775,1],\n",
        "\t[6.922596716,1.77106367,1],\n",
        "\t[8.675418651,-0.242068655,1],\n",
        "\t[7.673756466,3.508563011,1]]\n",
        "network = [[{'weights': [-1.5287693143012293, 2.0106573915414474, 1.1483508990090991]}, {'weights': [0.418898615690713, -0.2845611012887265, 0.20079984019463185]}],\n",
        "\t[{'weights': [2.9250983493424263, -0.5411417397536576, -0.9773407596066707]}, {'weights': [-2.908328762401596, 1.1926222711493746, 0.4523694752484928]}]]\n",
        "for row in dataset:\n",
        "\tprediction = predict(network, row)\n",
        "\tprint('Expected=%d, Got=%d' % (row[-1], prediction))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ANN.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "dc13f4eb4e94fef26ee5749c6d8c8c5515c5cee8ec0d0b78161a8841d04cc5fd"
    },
    "kernelspec": {
      "display_name": "Python 3.7.4 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
